# ==========================================
# EMBEDDING MODEL CONFIGURATION
# ==========================================
model_name: "BAAI/bge-base-en-v1.5"
model_dim: 768
device: "cuda"  # Options: "cpu", "cuda"
max_token_limit: 512

# Embedding generation batch size (increased from 32 to 128)
embedding_batch_size: 128

# Storage batch size for Qdrant uploads (increased from 100 to 500)
storage_batch_size: 500

# Use FP16/mixed precision for faster inference on GPU (ignored on CPU)
use_fp16: true

show_progress_bar: true

# ==========================================
# CHUNKING PARAMETERS
# ==========================================
overlap_tokens: 50 
use_sentence_boundaries: true  # Never split mid-sentence

include_header_path: true           # Add hierarchical section path

# ==========================================
# QDRANT CONFIGURATION
# ==========================================
qdrant_url: "http://localhost:6333"
collection_name: "markdown_chunk"
distance_metric: "Cosine"  # Options: "Cosine", "Euclid", "Dot"

grpc_port: 6334       # gRPC port (from docker-compose.yml)

# ==========================================
# LOGGING CONFIGURATION
# ==========================================
log_level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"