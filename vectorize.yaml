# ==========================================
# EMBEDDING MODEL CONFIGURATION
# ==========================================
model_name: "BAAI/bge-base-en-v1.5"
model_dim: 768
device: "cuda"  # Options: "cpu", "cuda"
max_token_limit: 512

# Embedding generation batch size (increased from 32 to 128)
embedding_batch_size: 128

# Storage batch size for Qdrant uploads (increased from 100 to 500)
storage_batch_size: 500

# Use FP16/mixed precision for faster inference on GPU (ignored on CPU)
use_fp16: true

# Show progress bar during embedding generation
show_progress_bar: true

# ==========================================
# CHUNKING PARAMETERS
# ==========================================
target_chunk_size: 512  # Target size in tokens (same as token limit)

# Semantic Chunking Behavior
keep_tables_intact: true      # Keep tables whole if possible
keep_code_blocks_intact: true # Keep code blocks whole if possible
keep_list_items_together: true # Keep list items together
use_sentence_boundaries: true  # Never split mid-sentence

# ==========================================
# CONTEXT ENHANCEMENT
# ==========================================
include_header_path: true           # Add hierarchical section path
include_surrounding_context: true   # Add sentences before/after
# surrounding_sentences_before: 2   # Sentences before current chunk
# surrounding_sentences_after: 1    # Sentences after current chunk

# ==========================================
# QDRANT CONFIGURATION
# ==========================================
qdrant_url: "http://localhost:6333"
collection_name: "markdown_chunk"
distance_metric: "Cosine"  # Options: "Cosine", "Euclid", "Dot"

grpc_port: 6334       # gRPC port (from docker-compose.yml)

# ==========================================
# LOGGING CONFIGURATION
# ==========================================
log_level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"