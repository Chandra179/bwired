# ==========================================
# EMBEDDING MODEL CONFIGURATION
# ==========================================
dense_model_name: "BAAI/bge-base-en-v1.5"
model_dim: 768
device: "cuda"  # Options: "cpu", "cuda"

embedding_token_limit: 512

# Embedding generation batch size (increased from 32 to 128)
embedding_batch_size: 128

# Storage batch size for Qdrant uploads (increased from 100 to 500)
storage_batch_size: 500

# Use FP16/mixed precision for faster inference on GPU (ignored on CPU)
use_fp16: true

show_progress_bar: true

# ==========================================
# CHUNKING PARAMETERS
# ==========================================
# Target size for each chunk (must be <= embedding_token_limit)
chunk_size: 400

# Overlap between adjacent chunks in the same section
overlap_tokens: 50 

# Never split mid-sentence when chunking text
use_sentence_boundaries: true

# Add hierarchical section path to chunks
include_header_path: true

# ==========================================
# QDRANT CONFIGURATION
# ==========================================
qdrant_url: "http://localhost:6333"
collection_name: "markdown_chunk"
distance_metric: "Cosine"  # Options: "Cosine", "Euclid", "Dot"

grpc_port: 6334       # gRPC port (from docker-compose.yml)

# ==========================================
# LOGGING CONFIGURATION
# ==========================================
log_level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR"